---
title: "Machine Learning 2 -  Homework 1"
author: "Tamas Koncz"
date: '2018-03-19'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, include=FALSE}
library(data.table)
library(dplyr)
library(stringr)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(gbm)
library(ISLR)
library(skimr)
library(ROCR)

options(scipen = 999)
theme_set(theme_bw())
```

### 1. Classification tree model (3 points)  
```{r}
data <- data.table(OJ)
```

A glimpse at the variables we will be working with:
```{r}
glimpse(data)
```


#### a) Create a training data of 75% and keep 25% of the data as a test set.  

```{r}
training_ratio <- 0.75

set.seed(93)
train_indices <- createDataPartition(y = data[["Purchase"]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]
```

#### b) Build a classification tree, determining the optimal complexity parameter via 10-fold cross validation.  
#####    • Use values for the complexity parameter ranging between 0.001 and 0.1.  
#####    • the selection criterion should be based on AUC  
#####    • Use the “one standard error” rule to select the final model  

```{r}
train_Control <- trainControl(method = "CV",
                              number = 10,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary,
                              selectionFunction = "oneSE")

tune_Grid_rpart <- data.frame(cp = seq(from = 0.001, to = 0.1, by = 0.005))
##I've shortened the tune_Grid so the results can be printed and still read.. the final model is the same.

set.seed(93)
rpart_fit <- train(Purchase ~ .,
                   data = data_train,
                   method = "rpart",
                   metric = "ROC",
                   trControl = train_Control,
                   tuneGrid = tune_Grid_rpart)
```

```{r}
rpart_fit
```

```{r}
ggplot(rpart_fit)
```

#### c) Plot the final model and interpret the result. How would you predict a new observation?  
  
```{r, fig.width=15, fig.height=10}
rpart.plot(rpart_fit[["finalModel"]])
```  

#### d) Evaluate the final model on the test set. Is the AUC close to what we got via cross-validation?  
```{r}
test_prob_rpart <- predict.train(rpart_fit, newdata = data_test, type = "prob")
test_pred_rpart <- prediction(test_prob_rpart$MM, data_test[["Purchase"]])

rpart_perf <- performance(test_pred_rpart, measure = "tpr", x.measure = "fpr")

rpart_roc_df <- data.table(
  model = "rpart",
  FPR = rpart_perf@x.values[[1]],
  TPR = rpart_perf@y.values[[1]],
  cutoff = rpart_perf@alpha.values[[1]]
)

#roc_df <- rbind(glm_roc_df, glmnet_roc_df)

ggplot(rpart_roc_df) +
  geom_line(aes(FPR, TPR, color = model), size = 2) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  xlab("False Positive Rate") + ylab("True Positive Rate")
```



### 2. Tree ensemble models (6 points)  

##### investigate tree ensemble models:
#####     • random forest
#####     • gradient boosting machine
#####     • XGBoost  

#### a) Try various tuning parameter combinations and select the best model using cross-validation. (This time when doing hyperparameter tuning, simply choose the best model instead of applying the oneSE rule.)  
```{r}
train_Control <- trainControl(method = "CV",
                              number = 10,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary,
                              selectionFunction = "best")
```

RF:
```{r}
tune_Grid_rf <- data.frame(mtry = c(2:7))

set.seed(93)
rf_100_fit <- train(Purchase ~ .,
                   data = data_train,
                   method = "rf",
                   metric = "ROC",
                   trControl = train_Control,
                   tuneGrid = tune_Grid_rf,
                   ntree = 100,
                   importance = T)

set.seed(93)
rf_250_fit <- train(Purchase ~ .,
                   data = data_train,
                   method = "rf",
                   metric = "ROC",
                   trControl = train_Control,
                   tuneGrid = tune_Grid_rf,
                   ntree = 250,
                   importance = T)

set.seed(93)
rf_500_fit <- train(Purchase ~ .,
                   data = data_train,
                   method = "rf",
                   metric = "ROC",
                   trControl = train_Control,
                   tuneGrid = tune_Grid_rf ,
                   ntree = 500,
                   importance = T)
```

GBM:
```{r}
tune_Grid_gbm <- expand.grid(n.trees = c(100, 500, 1000), 
                        interaction.depth = c(2, 3, 5), 
                        shrinkage = c(0.005, 0.01, 0.1),
                        n.minobsinnode = c(5, 10))
  
set.seed(93)
gbm_025_fit <- train(Purchase ~ .,
                   method = "gbm",
                   metric = "ROC",
                   data = data_train,
                   trControl = train_Control,
                   tuneGrid = tune_Grid_gbm,
                   bag.fraction = 0.25,
                   verbose = FALSE
                   )

set.seed(93)
gbm_050_fit <- train(Purchase ~ .,
                   method = "gbm",
                   metric = "ROC",
                   data = data_train,
                   trControl = train_Control,
                   tuneGrid = tune_Grid_gbm,
                   bag.fraction = 0.50,
                   verbose = FALSE
                   )

set.seed(93)
gbm_075_fit <- train(Purchase ~ .,
                   method = "gbm",
                   metric = "ROC",
                   data = data_train,
                   trControl = train_Control,
                   tuneGrid = tune_Grid_gbm,
                   bag.fraction = 0.75,
                   verbose = FALSE
                   )
```

XGBoost:
```{r}
tune_Grid_xgb <- expand.grid(nrounds = c(500, 1000), max_depth = c(2, 3, 5),
                             eta = c(0.01, 0.05), gamma = 0,
                             colsample_bytree = c(0.5, 0.7), min_child_weight = 1, 
                             subsample = c(0.5))
set.seed(93)
xgboost_fit <- train(Purchase ~ .,
                      method = "xgbTree",
                      metric = "ROC",
                      data = data_train,
                      trControl = train_Control,
                      tuneGrid = tune_Grid_xgb)
xgboost_fit
```

#### b) Compare different models with the resamples function (make sure to set the same seed before model training for all 3 models). Is any of these giving significantly different predictive power than the others?  

```{r, fig.width=15, fig.height=6}
resamples_object <- resamples(list("rpart" = rpart_fit,
                                   "rf - 100" = rf_100_fit,
                                   "rf - 250" = rf_250_fit,
                                   "rf - 500" = rf_500_fit,
                                   "gbm = .25" = gbm_025_fit,
                                   "gbm = .50" = gbm_050_fit,
                                   "gbm = .75" = gbm_075_fit,
                                   "xgboost" = xgboost_fit))
##TODO: come up with a nice viz...
resamples_object$values %>%
  tidyr::gather(key= "Resample", factor_key = F) %>%
  setnames(c("Fold", "Model~Metric", "Value")) %>%
  mutate(model = str_split(`Model~Metric`, "~", simplify = T)[,1],
         metric = str_split(`Model~Metric`, "~", simplify = T)[,2]) %>%
  mutate(model = factor(model, levels = c("rpart", 
                                          "rf - 100", "rf - 250", "rf - 500",
                                          "gbm = .25", "gbm = .50", "gbm = .75", 
                                          "xgboost"))) %>%
  ggplot(aes(x= model, y= Value, fill = model)) +
    geom_boxplot() +
    facet_grid(~metric)

```


#### c) Choose the best model and plot ROC curve for the best model on the test set. Calculate and interpret AUC.  

```{r}
test_prob_rpart <- predict.train(rpart_fit, newdata = data_test, type = "prob")
test_pred_rpart <- prediction(test_prob_rpart$MM, data_test[["Purchase"]])

rpart_perf <- performance(test_pred_rpart, measure = "tpr", x.measure = "fpr")

rpart_roc_df <- data.table(
  model = "rpart",
  FPR = rpart_perf@x.values[[1]],
  TPR = rpart_perf@y.values[[1]],
  cutoff = rpart_perf@alpha.values[[1]]
)

#roc_df <- rbind(glm_roc_df, glmnet_roc_df)

ggplot(rpart_roc_df) +
  geom_line(aes(FPR, TPR, color = model), size = 2) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  xlab("False Positive Rate") + ylab("True Positive Rate")
```


#### d) Inspect variable importance plots for the 3 models. Are similar variables found to be the most important for the 3 models?  

### 3. Variable importance profiles (4 points)

```{r}
data <- data.table(Hitters)
data <- data[!is.na(Salary)]
data[, log_salary := log(Salary)]
data[, Salary := NULL]
```

#### a) train two random forest models: one with mtry = 2 and another with mtry = 10 (use the whole dataset and don’t use cross-validation). Inspect variable importance profiles. What do you see in terms of how important the first few variables are relative to each other?
#### b) One of them is more extreme in terms of how the most important and the next ones relate to each other. Give an intuitive explanation how mtry relates to relative importance of variables in random forest models.
#### c) In the same vein, estimate two gbm models and set bag.fraction to 0.1 first and to 0.9 in the second.
##### The tuneGrid should consist of the same values for the two models (a dataframe with one row):  
#####     • n.trees = 500  
#####     • interaction.depth = 5  
#####     • shrinkage = 0.1  
#####     • n.minobsinnode = 5  
##### Compare variable importance plots for the two models. What is the meaning of bag.fraction? Based on this, why is one variable importance profile more extreme than the other?  
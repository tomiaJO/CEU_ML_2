---
title: "Machine Learning 2 -  Homework 2"
author: "Tamas Koncz"
date: '2018-04-03'
output:
  html_document:
    df_print: paged
  html_notebook:
    df_print: paged
---

```{r, include = FALSE}
library(data.table)
library(dplyr)
library(GGally)
library(h2o)

library(knitr)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

options(scipen = 999)
```

```{r}
data <- fread("no-show-data.csv")
# data <- fread("../../data/medical-appointments-no-show/no-show-data.csv")

# some data cleaning
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(data,
         c("No-show",
           "Age",
           "Gender",
           "ScheduledDay",
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"),
         c("no_show",
           "age",
           "gender",
           "scheduled_day",
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))

# for binary prediction, the target variable must be a factor
data[, no_show := factor(no_show, levels = c("Yes", "No"))]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# create new variables
data[, gender := factor(gender)]

data[, scholarship := factor(scholarship)]
data[, hypertension := factor(hypertension)]
data[, alcoholism := factor(alcoholism)]
data[, handicap := factor(handicap)]
data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]

# clean up a little bit
data <- data[age %between% c(0, 95)]
data <- data[days_since_scheduled > -1]
data[, c("scheduled_day", "appointment_day", "sms_received") := NULL]
```

```{r}
h2o.init()
data <- as.h2o(data)
```


#### 1. Deep learning with h2o (7 points)
##### Please for all models you are building, use reproducible = TRUE option so that conclusions that you draw are not dependent on the particular run of your models. Also, please set the same seed.  

##### a) Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts.  

```{r}
splitted_data <- h2o.splitFrame(data, 
                                ratios = c(0.05, 0.45), 
                                seed = 123)

data_train <- splitted_data[[1]]
data_valid <- splitted_data[[2]]
data_test <- splitted_data[[3]]
```


##### b) Train a benchmark model of your choice using h2o (such as random forest, gbm or glm) and evaluate it on the validation set.  

Let's train a basic RF model, with 500 trees and a grid search over a few possible values for mtry (# of parameters to chose from randomly at splitting):
```{r}
y <- "no_show"
X <- setdiff(names(data), y)

rf_params <- list(ntrees = c(500),
                  mtries = c(2, 3, 5))

rf_grid <- h2o.grid(x = X, 
                    y = y, 
                    training_frame = data_train, 
                    algorithm = "randomForest", 
                    nfolds = 5,
                    seed = 123,
                    hyper_params = rf_params)
```

##### c) Build deep learning models. Experiment with parameter settings regarding  

• network topology (varying number of layers and nodes within layers)  
• activation function  
• dropout (both hidden and input layers)  
• lasso, ridge regularization  
• early stopping (changing stopping rounds, tolerance) and number of epochs  

For all models, supply the validation_frame and use AUC as a stopping metric.  
Present different model versions and evaluate them on the validation set. Which one performs the best?  

For this exercise, I had to slice up the search grid into two (hyperparams_1 & 2) for runtime consideration (also the tried ranges for parameters are quite limited). This probably creates an under-optimal performance for the DL models.
```{r}
# dl_grid <- h2o.deeplearning(X, y, 
#                             training_frame = data_train,
#                             reproducible = TRUE, # needed in deeplearning for full reproducibility
#                             seed = 123)

hyperparams_1 <- list(
                  activation = c("TanhWithDropout", "RectifierWithDropout"),
                  input_dropout_ratio = c(0, 0.15),
                  hidden_dropout_ratios = list(c(0, 0), c(0.15, 0.15)),
                  hidden = list(c(32, 32), c(50, 50))
                  )

hyperparams_2 <- list(
                  l1 = c(0, 0.05),
                  l2 = c(0, 0.05),
                  epochs = c(6, 8),
                  stopping_rounds = c(2, 4)
                  )

dl_grid_1 <- h2o.grid(
                  x=X, y=y,
                  algorithm = "deeplearning",
                  reproducible = TRUE,
                  stopping_metric = "AUC",
                  hyper_params = hyperparams_1,
                  training_frame = data_train,
                  validation_frame = data_valid,
                  seed = 123
                  )

dl_grid_2 <- h2o.grid(
                  x=X, y=y,
                  algorithm = "deeplearning",
                  reproducible = TRUE,
                  stopping_metric = "AUC",
                  hyper_params = hyperparams_2,
                  training_frame = data_train,
                  validation_frame = data_valid,
                  seed = 123
                  )
```

Version for different settings (commented out for now as prints too much):
```{r}
# h2o.getGrid(grid_id = rf_grid@grid_id, sort_by = "AUC", decreasing = TRUE)
# h2o.getGrid(grid_id = dl_grid_1@grid_id, sort_by = "AUC", decreasing = TRUE)
# h2o.getGrid(grid_id = dl_grid_2@grid_id, sort_by = "AUC", decreasing = TRUE)
```


##### d) How does your best model compare to the benchmark model on the test set?  

Let's extract the best models first from the grids:
```{r}
dl_model_1  <- h2o.getModel(h2o.getGrid(dl_grid_1@grid_id)@model_ids[[1]])
dl_model_2  <- h2o.getModel(h2o.getGrid(dl_grid_2@grid_id)@model_ids[[1]])
```

Best model AUC from the first set of parameters:
```{r}
print(h2o.auc(h2o.performance(dl_model_1, newdata = data_valid)))
```

Best model AUC from the second set of parameters:
```{r}
print(h2o.auc(h2o.performance(dl_model_2, newdata = data_valid)))
```

The results from the first set are slighlty better (although, realistically they are virtually the same), so I'm going to use that for comparison against the benchmark.  
  

DL's performance on the test set:
```{r}
h2o.auc(h2o.performance(dl_model_1, newdata = data_test))
```

Performance of the benchmark RF model on the test set:
```{r}
rf_model  <- h2o.getModel(h2o.getGrid(rf_grid@grid_id)@model_ids[[1]])
h2o.auc(h2o.performance(rf_model, newdata = data_test))
```

A bit surprisingly for me, the model that comes out on top is the DL one (with the first parameter set).  
The performance is a tiny bit better than RF had on the test set, while DL was very consistent across the three dataset's (performance on the training set was also around 0.706 in terms of AUC).  
  
##### e) Evaluate the model that performs best based on the validation set on the test set.  

```{r}
h2o.performance(dl_model_1, newdata = data_test)
```

Results are mixed - accuracy is not actually far from what we would get by just random guessing based on base probabilities.  

~ 0.7 AUC however is definitely a good uplift for random guessing, and it's in the ballpark of what we could achieve with the potentially more "appropriate" model (for this problem) in ML1 HW2 with glmnet.

#### 2. Stacking with h2o (6 points)  

##### Take the same problem and data splits.  

Data splits are still available for exercise 1.

##### a) Build at least 4 models of different families using cross validation, keeping cross validated predictions.  

```{r}
glm_model           <- h2o.glm(
                            X, y,
                            training_frame = data_train,
                            family = "binomial",
                            alpha = 1, 
                            lambda_search = TRUE,
                            seed = 123,
                            nfolds = 5, 
                            keep_cross_validation_predictions = TRUE
                          )

rpart_model         <- h2o.gbm(
                            X, y,
                            training_frame = data_train,
                            ntrees = 1, 
                            max_depth = 10, 
                            learn_rate = 0.1, 
                            seed = 123,
                            nfolds = 5,
                            keep_cross_validation_predictions = TRUE
                          )

gbm_model           <- h2o.gbm(
                            X, y,
                            training_frame = data_train,
                            ntrees = 200, 
                            max_depth = 10, 
                            learn_rate = 0.1, 
                            seed = 123,
                            nfolds = 5,
                            keep_cross_validation_predictions = TRUE
                          )

deeplearning_model  <- h2o.deeplearning(
                            X, y,
                            training_frame = data_train,
                            hidden = c(32, 8),
                            seed = 123,
                            nfolds = 5, 
                            keep_cross_validation_predictions = TRUE
                          )
```


##### b) Evaluate validation set performance of each model.  

Let's see the AUC for each model on the validation set:
```{r}
print(paste("glm:"  , round(h2o.auc(h2o.performance(glm_model,          newdata = data_valid)), 3)))
print(paste("rpart:", round(h2o.auc(h2o.performance(rpart_model,        newdata = data_valid)), 3)))
print(paste("gbm:"  , round(h2o.auc(h2o.performance(gbm_model,          newdata = data_valid)), 3)))
print(paste("dl:"   , round(h2o.auc(h2o.performance(deeplearning_model, newdata = data_valid)), 3)))
```


##### c) How large are the correlations of predicted scores of the validation set produced by the base learners?  

```{r}
predictions <- data.table(
  "glm"   = as.data.frame(h2o.predict(glm_model,           newdata = data_test)$Y)$Y,
  "rpart" = as.data.frame(h2o.predict(rpart_model,         newdata = data_test)$Y)$Y,
  "gbm"   = as.data.frame(h2o.predict(gbm_model,           newdata = data_test)$Y)$Y,
  "dl"    = as.data.frame(h2o.predict(deeplearning_model,  newdata = data_test)$Y)$Y
)

ggcorr(predictions, label = TRUE, label_round = 2)
```

Correlations vary across models - not surprisingly, it's highest between the tree-based methods.
It's also significant between glm and dl (and between dl and the other two methods as well).

##### d) Create a stacked ensemble model from the base learners. Experiment with at least two different ensembling meta learners. 

Let's start with the base glm:
```{r}
ensemble_model_glm <- h2o.stackedEnsemble(
                              X, y,
                              training_frame = data_train,
                              metalearner_algorithm = "glm",
                              base_models = list(glm_model,
                                                 rpart_model,
                                                 gbm_model,
                                                 deeplearning_model))
```

And then try an rf-based ensemble as well:
```{r}
ensemble_model_rf  <- h2o.stackedEnsemble(
                              X, y,
                              training_frame = data_train,
                              metalearner_algorithm = "drf",
                              base_models = list(glm_model,
                                                 rpart_model,
                                                 gbm_model,
                                                 deeplearning_model))
```

##### e) Evaluate ensembles on validation set. Did it improve prediction?  

Based on AUCs:
```{r}
print(paste("glm ensemble:" , round(h2o.auc(h2o.performance(ensemble_model_glm,  newdata = data_valid)), 3)))
print(paste("rf ensemble:"  , round(h2o.auc(h2o.performance(ensemble_model_rf,   newdata = data_valid)), 3)))
```

Well, the glm version did - not by very much, but still the difference is significant enough to call it.
RF performed worse than most models.. Maybe random selection hurt in this case?

##### f) Evaluate the best performing model on the test set. How does performance compare to that of the validation set?  

AUC on the test set, for the glm ensemble:
```{r}
print(paste("glm ensemble:" , round(h2o.auc(h2o.performance(ensemble_model_glm,  newdata = data_test)), 3)))
```

The result is close enough to call it an even performance. The 0.711 AUC by the glm ensemble is still better than what we've seen by any other model on the validation sets earlier.

```{r}
sessionInfo()
```

